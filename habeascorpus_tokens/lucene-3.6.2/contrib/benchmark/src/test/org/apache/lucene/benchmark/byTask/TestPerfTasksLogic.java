/** * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */	TokenNameCOMMENT_JAVADOC	 Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at * http://www.apache.org/licenses/LICENSE-2.0 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. 
package	TokenNamepackage	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
io	TokenNameIdentifier	 io
.	TokenNameDOT	
BufferedReader	TokenNameIdentifier	 Buffered Reader
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
io	TokenNameIdentifier	 io
.	TokenNameDOT	
File	TokenNameIdentifier	 File
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
io	TokenNameIdentifier	 io
.	TokenNameDOT	
FileReader	TokenNameIdentifier	 File Reader
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
io	TokenNameIdentifier	 io
.	TokenNameDOT	
StringReader	TokenNameIdentifier	 String Reader
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
text	TokenNameIdentifier	 text
.	TokenNameDOT	
Collator	TokenNameIdentifier	 Collator
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
util	TokenNameIdentifier	 util
.	TokenNameDOT	
List	TokenNameIdentifier	 List
;	TokenNameSEMICOLON	
import	TokenNameimport	
java	TokenNameIdentifier	 java
.	TokenNameDOT	
util	TokenNameIdentifier	 util
.	TokenNameDOT	
Locale	TokenNameIdentifier	 Locale
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
analysis	TokenNameIdentifier	 analysis
.	TokenNameDOT	
Analyzer	TokenNameIdentifier	 Analyzer
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
analysis	TokenNameIdentifier	 analysis
.	TokenNameDOT	
BaseTokenStreamTestCase	TokenNameIdentifier	 Base Token Stream Test Case
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
analysis	TokenNameIdentifier	 analysis
.	TokenNameDOT	
MockAnalyzer	TokenNameIdentifier	 Mock Analyzer
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
analysis	TokenNameIdentifier	 analysis
.	TokenNameDOT	
TokenStream	TokenNameIdentifier	 Token Stream
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
analysis	TokenNameIdentifier	 analysis
.	TokenNameDOT	
tokenattributes	TokenNameIdentifier	 tokenattributes
.	TokenNameDOT	
CharTermAttribute	TokenNameIdentifier	 Char Term Attribute
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
BenchmarkTestCase	TokenNameIdentifier	 Benchmark Test Case
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
.	TokenNameDOT	
feeds	TokenNameIdentifier	 feeds
.	TokenNameDOT	
DocMaker	TokenNameIdentifier	 Doc Maker
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
.	TokenNameDOT	
feeds	TokenNameIdentifier	 feeds
.	TokenNameDOT	
ReutersQueryMaker	TokenNameIdentifier	 Reuters Query Maker
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
.	TokenNameDOT	
stats	TokenNameIdentifier	 stats
.	TokenNameDOT	
TaskStats	TokenNameIdentifier	 Task Stats
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
.	TokenNameDOT	
tasks	TokenNameIdentifier	 tasks
.	TokenNameDOT	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
.	TokenNameDOT	
tasks	TokenNameIdentifier	 tasks
.	TokenNameDOT	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
byTask	TokenNameIdentifier	 by Task
.	TokenNameDOT	
tasks	TokenNameIdentifier	 tasks
.	TokenNameDOT	
WriteLineDocTask	TokenNameIdentifier	 Write Line Doc Task
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
collation	TokenNameIdentifier	 collation
.	TokenNameDOT	
CollationKeyAnalyzer	TokenNameIdentifier	 Collation Key Analyzer
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
facet	TokenNameIdentifier	 facet
.	TokenNameDOT	
taxonomy	TokenNameIdentifier	 taxonomy
.	TokenNameDOT	
TaxonomyReader	TokenNameIdentifier	 Taxonomy Reader
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
IndexReader	TokenNameIdentifier	 Index Reader
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
IndexWriter	TokenNameIdentifier	 Index Writer
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
LogDocMergePolicy	TokenNameIdentifier	 Log Doc Merge Policy
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
LogMergePolicy	TokenNameIdentifier	 Log Merge Policy
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
Term	TokenNameIdentifier	 Term
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
TermEnum	TokenNameIdentifier	 Term Enum
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
TermDocs	TokenNameIdentifier	 Term Docs
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
SegmentInfos	TokenNameIdentifier	 Segment Infos
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
SerialMergeScheduler	TokenNameIdentifier	 Serial Merge Scheduler
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
TermFreqVector	TokenNameIdentifier	 Term Freq Vector
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
index	TokenNameIdentifier	 index
.	TokenNameDOT	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
.	TokenNameDOT	
OpenMode	TokenNameIdentifier	 Open Mode
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
store	TokenNameIdentifier	 store
.	TokenNameDOT	
Directory	TokenNameIdentifier	 Directory
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
search	TokenNameIdentifier	 search
.	TokenNameDOT	
FieldCache	TokenNameIdentifier	 Field Cache
.	TokenNameDOT	
StringIndex	TokenNameIdentifier	 String Index
;	TokenNameSEMICOLON	
import	TokenNameimport	
org	TokenNameIdentifier	 org
.	TokenNameDOT	
apache	TokenNameIdentifier	 apache
.	TokenNameDOT	
lucene	TokenNameIdentifier	 lucene
.	TokenNameDOT	
search	TokenNameIdentifier	 search
.	TokenNameDOT	
FieldCache	TokenNameIdentifier	 Field Cache
;	TokenNameSEMICOLON	
/** * Test very simply that perf tasks - simple algorithms - are doing what they should. */	TokenNameCOMMENT_JAVADOC	 Test very simply that perf tasks - simple algorithms - are doing what they should. 
public	TokenNamepublic	
class	TokenNameclass	
TestPerfTasksLogic	TokenNameIdentifier	 Test Perf Tasks Logic
extends	TokenNameextends	
BenchmarkTestCase	TokenNameIdentifier	 Benchmark Test Case
{	TokenNameLBRACE	
@	TokenNameAT	
Override	TokenNameIdentifier	 Override
public	TokenNamepublic	
void	TokenNamevoid	
setUp	TokenNameIdentifier	 set Up
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
super	TokenNamesuper	
.	TokenNameDOT	
setUp	TokenNameIdentifier	 set Up
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
copyToWorkDir	TokenNameIdentifier	 copy To Work Dir
(	TokenNameLPAREN	
"reuters.first20.lines.txt"	TokenNameStringLiteral	reuters.first20.lines.txt
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test index creation logic */	TokenNameCOMMENT_JAVADOC	 Test index creation logic 
public	TokenNamepublic	
void	TokenNamevoid	
testIndexAndSearchTasks	TokenNameIdentifier	 test Index And Search Tasks
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : 1000"	TokenNameStringLiteral	{ AddDoc } : 1000
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader"	TokenNameStringLiteral	OpenReader
,	TokenNameCOMMA	
"{ CountingSearchTest } : 200"	TokenNameStringLiteral	{ CountingSearchTest } : 200
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
"[ CountingSearchTest > : 70"	TokenNameStringLiteral	[ CountingSearchTest > : 70
,	TokenNameCOMMA	
"[ CountingSearchTest > : 9"	TokenNameStringLiteral	[ CountingSearchTest > : 9
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. we test this value later 	TokenNameCOMMENT_LINE	2. we test this value later 
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// 3. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	3. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 4. test specific checks after the benchmark run completed. 	TokenNameCOMMENT_LINE	4. test specific checks after the benchmark run completed. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"TestSearchTask was supposed to be called!"	TokenNameStringLiteral	TestSearchTask was supposed to be called!
,	TokenNameCOMMA	
279	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"Index does not exist?...!"	TokenNameStringLiteral	Index does not exist?...!
,	TokenNameCOMMA	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
indexExists	TokenNameIdentifier	 index Exists
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// now we should be able to open the index for write. 	TokenNameCOMMENT_LINE	now we should be able to open the index for write. 
IndexWriter	TokenNameIdentifier	 Index Writer
iw	TokenNameIdentifier	 iw
=	TokenNameEQUAL	
new	TokenNamenew	
IndexWriter	TokenNameIdentifier	 Index Writer
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
new	TokenNamenew	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
(	TokenNameLPAREN	
TEST_VERSION_CURRENT	TokenNameIdentifier	 TEST  VERSION  CURRENT
,	TokenNameCOMMA	
new	TokenNamenew	
MockAnalyzer	TokenNameIdentifier	 Mock Analyzer
(	TokenNameLPAREN	
random	TokenNameIdentifier	 random
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
setOpenMode	TokenNameIdentifier	 set Open Mode
(	TokenNameLPAREN	
OpenMode	TokenNameIdentifier	 Open Mode
.	TokenNameDOT	
APPEND	TokenNameIdentifier	 APPEND
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
iw	TokenNameIdentifier	 iw
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"1000 docs were added to the index, this is what we expect to find!"	TokenNameStringLiteral	1000 docs were added to the index, this is what we expect to find!
,	TokenNameCOMMA	
1000	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test timed sequence task. */	TokenNameCOMMENT_JAVADOC	 Test timed sequence task. 
public	TokenNamepublic	
void	TokenNamevoid	
testTimedSearchTask	TokenNameIdentifier	 test Timed Search Task
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"log.step=100000"	TokenNameStringLiteral	log.step=100000
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : 100"	TokenNameStringLiteral	{ AddDoc } : 100
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader"	TokenNameStringLiteral	OpenReader
,	TokenNameCOMMA	
"{ CountingSearchTest } : .5s"	TokenNameStringLiteral	{ CountingSearchTest } : .5s
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
>	TokenNameGREATER	
0	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
long	TokenNamelong	
elapsed	TokenNameIdentifier	 elapsed
=	TokenNameEQUAL	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
prevLastMillis	TokenNameIdentifier	 prev Last Millis
-	TokenNameMINUS	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
startMillis	TokenNameIdentifier	 start Millis
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"elapsed time was "	TokenNameStringLiteral	elapsed time was 
+	TokenNamePLUS	
elapsed	TokenNameIdentifier	 elapsed
+	TokenNamePLUS	
" msec"	TokenNameStringLiteral	 msec
,	TokenNameCOMMA	
elapsed	TokenNameIdentifier	 elapsed
<=	TokenNameLESS_EQUAL	
1500	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
// disabled until we fix BG thread prio -- this test 	TokenNameCOMMENT_LINE	disabled until we fix BG thread prio -- this test 
// causes build to hang 	TokenNameCOMMENT_LINE	causes build to hang 
public	TokenNamepublic	
void	TokenNamevoid	
testBGSearchTaskThreads	TokenNameIdentifier	 test BG Search Task Threads
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"log.time.step.msec = 100"	TokenNameStringLiteral	log.time.step.msec = 100
,	TokenNameCOMMA	
"log.step=100000"	TokenNameStringLiteral	log.step=100000
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : 1000"	TokenNameStringLiteral	{ AddDoc } : 1000
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader"	TokenNameStringLiteral	OpenReader
,	TokenNameCOMMA	
"{"	TokenNameStringLiteral	{
,	TokenNameCOMMA	
" [ "XSearch" { CountingSearchTest > : * ] : 2 &-1"	TokenNameStringLiteral	 [ "XSearch" { CountingSearchTest > : * ] : 2 &-1
,	TokenNameCOMMA	
" Wait(0.5)"	TokenNameStringLiteral	 Wait(0.5)
,	TokenNameCOMMA	
"}"	TokenNameStringLiteral	}
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
"RepSumByPref X"	TokenNameStringLiteral	RepSumByPref X
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
>	TokenNameGREATER	
0	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
public	TokenNamepublic	
void	TokenNamevoid	
testHighlighting	TokenNameIdentifier	 test Highlighting
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"doc.stored=true"	TokenNameStringLiteral	doc.stored=true
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"query.maker="	TokenNameStringLiteral	query.maker=
+	TokenNamePLUS	
ReutersQueryMaker	TokenNameIdentifier	 Reuters Query Maker
.	TokenNameDOT	
class	TokenNameclass	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : 100"	TokenNameStringLiteral	{ AddDoc } : 100
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader(true)"	TokenNameStringLiteral	OpenReader(true)
,	TokenNameCOMMA	
"{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200"	TokenNameStringLiteral	{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. we test this value later 	TokenNameCOMMENT_LINE	2. we test this value later 
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// 3. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	3. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 4. test specific checks after the benchmark run completed. 	TokenNameCOMMENT_LINE	4. test specific checks after the benchmark run completed. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"TestSearchTask was supposed to be called!"	TokenNameStringLiteral	TestSearchTask was supposed to be called!
,	TokenNameCOMMA	
92	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
//pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs 	TokenNameCOMMENT_LINE	pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs 
//we probably should use a different doc/query maker, but... 	TokenNameCOMMENT_LINE	we probably should use a different doc/query maker, but... 
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"TestSearchTask was supposed to be called!"	TokenNameStringLiteral	TestSearchTask was supposed to be called!
,	TokenNameCOMMA	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
>=	TokenNameGREATER_EQUAL	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
&&	TokenNameAND_AND	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
>	TokenNameGREATER	
0	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"Index does not exist?...!"	TokenNameStringLiteral	Index does not exist?...!
,	TokenNameCOMMA	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
indexExists	TokenNameIdentifier	 index Exists
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// now we should be able to open the index for write. 	TokenNameCOMMENT_LINE	now we should be able to open the index for write. 
IndexWriter	TokenNameIdentifier	 Index Writer
iw	TokenNameIdentifier	 iw
=	TokenNameEQUAL	
new	TokenNamenew	
IndexWriter	TokenNameIdentifier	 Index Writer
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
new	TokenNamenew	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
(	TokenNameLPAREN	
TEST_VERSION_CURRENT	TokenNameIdentifier	 TEST  VERSION  CURRENT
,	TokenNameCOMMA	
new	TokenNamenew	
MockAnalyzer	TokenNameIdentifier	 Mock Analyzer
(	TokenNameLPAREN	
random	TokenNameIdentifier	 random
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
setOpenMode	TokenNameIdentifier	 set Open Mode
(	TokenNameLPAREN	
OpenMode	TokenNameIdentifier	 Open Mode
.	TokenNameDOT	
APPEND	TokenNameIdentifier	 APPEND
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
iw	TokenNameIdentifier	 iw
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"100 docs were added to the index, this is what we expect to find!"	TokenNameStringLiteral	100 docs were added to the index, this is what we expect to find!
,	TokenNameCOMMA	
100	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
public	TokenNamepublic	
void	TokenNamevoid	
testHighlightingTV	TokenNameIdentifier	 test Highlighting TV
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"doc.stored=true"	TokenNameStringLiteral	doc.stored=true
,	TokenNameCOMMA	
//doc storage is required in order to have text to highlight 	TokenNameCOMMENT_LINE	doc storage is required in order to have text to highlight 
"doc.term.vector.offsets=true"	TokenNameStringLiteral	doc.term.vector.offsets=true
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"query.maker="	TokenNameStringLiteral	query.maker=
+	TokenNamePLUS	
ReutersQueryMaker	TokenNameIdentifier	 Reuters Query Maker
.	TokenNameDOT	
class	TokenNameclass	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : 1000"	TokenNameStringLiteral	{ AddDoc } : 1000
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader(false)"	TokenNameStringLiteral	OpenReader(false)
,	TokenNameCOMMA	
"{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200"	TokenNameStringLiteral	{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. we test this value later 	TokenNameCOMMENT_LINE	2. we test this value later 
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// 3. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	3. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 4. test specific checks after the benchmark run completed. 	TokenNameCOMMENT_LINE	4. test specific checks after the benchmark run completed. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"TestSearchTask was supposed to be called!"	TokenNameStringLiteral	TestSearchTask was supposed to be called!
,	TokenNameCOMMA	
92	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
//pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs 	TokenNameCOMMENT_LINE	pretty hard to figure out a priori how many docs are going to have highlighted fragments returned, but we can never have more than the number of docs 
//we probably should use a different doc/query maker, but... 	TokenNameCOMMENT_LINE	we probably should use a different doc/query maker, but... 
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"TestSearchTask was supposed to be called!"	TokenNameStringLiteral	TestSearchTask was supposed to be called!
,	TokenNameCOMMA	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
>=	TokenNameGREATER_EQUAL	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
&&	TokenNameAND_AND	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
>	TokenNameGREATER	
0	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"Index does not exist?...!"	TokenNameStringLiteral	Index does not exist?...!
,	TokenNameCOMMA	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
indexExists	TokenNameIdentifier	 index Exists
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// now we should be able to open the index for write. 	TokenNameCOMMENT_LINE	now we should be able to open the index for write. 
IndexWriter	TokenNameIdentifier	 Index Writer
iw	TokenNameIdentifier	 iw
=	TokenNameEQUAL	
new	TokenNamenew	
IndexWriter	TokenNameIdentifier	 Index Writer
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
new	TokenNamenew	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
(	TokenNameLPAREN	
TEST_VERSION_CURRENT	TokenNameIdentifier	 TEST  VERSION  CURRENT
,	TokenNameCOMMA	
new	TokenNamenew	
MockAnalyzer	TokenNameIdentifier	 Mock Analyzer
(	TokenNameLPAREN	
random	TokenNameIdentifier	 random
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
setOpenMode	TokenNameIdentifier	 set Open Mode
(	TokenNameLPAREN	
OpenMode	TokenNameIdentifier	 Open Mode
.	TokenNameDOT	
APPEND	TokenNameIdentifier	 APPEND
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
iw	TokenNameIdentifier	 iw
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"1000 docs were added to the index, this is what we expect to find!"	TokenNameStringLiteral	1000 docs were added to the index, this is what we expect to find!
,	TokenNameCOMMA	
1000	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
public	TokenNamepublic	
void	TokenNamevoid	
testHighlightingNoTvNoStore	TokenNameIdentifier	 test Highlighting No Tv No Store
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"query.maker="	TokenNameStringLiteral	query.maker=
+	TokenNamePLUS	
ReutersQueryMaker	TokenNameIdentifier	 Reuters Query Maker
.	TokenNameDOT	
class	TokenNameclass	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : 1000"	TokenNameStringLiteral	{ AddDoc } : 1000
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader"	TokenNameStringLiteral	OpenReader
,	TokenNameCOMMA	
"{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200"	TokenNameStringLiteral	{ CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body]) } : 200
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. we test this value later 	TokenNameCOMMENT_LINE	2. we test this value later 
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numHighlightedResults	TokenNameIdentifier	 num Highlighted Results
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
CountingHighlighterTestTask	TokenNameIdentifier	 Counting Highlighter Test Task
.	TokenNameDOT	
numDocsRetrieved	TokenNameIdentifier	 num Docs Retrieved
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// 3. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	3. execute the algorithm (required in every "logic" test) 
try	TokenNametry	
{	TokenNameLBRACE	
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"CountingHighlighterTest should have thrown an exception"	TokenNameStringLiteral	CountingHighlighterTest should have thrown an exception
,	TokenNameCOMMA	
false	TokenNamefalse	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertNotNull	TokenNameIdentifier	 assert Not Null
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// (avoid compile warning on unused variable) 	TokenNameCOMMENT_LINE	(avoid compile warning on unused variable) 
}	TokenNameRBRACE	
catch	TokenNamecatch	
(	TokenNameLPAREN	
Exception	TokenNameIdentifier	 Exception
e	TokenNameIdentifier	 e
)	TokenNameRPAREN	
{	TokenNameLBRACE	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
/** * Test Exhasting Doc Maker logic */	TokenNameCOMMENT_JAVADOC	 Test Exhasting Doc Maker logic 
public	TokenNamepublic	
void	TokenNamevoid	
testExhaustContentSource	TokenNameIdentifier	 test Exhaust Content Source
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
,	TokenNameCOMMA	
"content.source.log.step=1"	TokenNameStringLiteral	content.source.log.step=1
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ AddDoc } : * "	TokenNameStringLiteral	{ AddDoc } : * 
,	TokenNameCOMMA	
"ForceMerge(1)"	TokenNameStringLiteral	ForceMerge(1)
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"OpenReader"	TokenNameStringLiteral	OpenReader
,	TokenNameCOMMA	
"{ CountingSearchTest } : 100"	TokenNameStringLiteral	{ CountingSearchTest } : 100
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
,	TokenNameCOMMA	
"[ CountingSearchTest > : 30"	TokenNameStringLiteral	[ CountingSearchTest > : 30
,	TokenNameCOMMA	
"[ CountingSearchTest > : 9"	TokenNameStringLiteral	[ CountingSearchTest > : 9
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. we test this value later 	TokenNameCOMMENT_LINE	2. we test this value later 
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// 3. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	3. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 4. test specific checks after the benchmark run completed. 	TokenNameCOMMENT_LINE	4. test specific checks after the benchmark run completed. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"TestSearchTask was supposed to be called!"	TokenNameStringLiteral	TestSearchTask was supposed to be called!
,	TokenNameCOMMA	
139	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"Index does not exist?...!"	TokenNameStringLiteral	Index does not exist?...!
,	TokenNameCOMMA	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
indexExists	TokenNameIdentifier	 index Exists
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// now we should be able to open the index for write. 	TokenNameCOMMENT_LINE	now we should be able to open the index for write. 
IndexWriter	TokenNameIdentifier	 Index Writer
iw	TokenNameIdentifier	 iw
=	TokenNameEQUAL	
new	TokenNamenew	
IndexWriter	TokenNameIdentifier	 Index Writer
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
new	TokenNamenew	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
(	TokenNameLPAREN	
TEST_VERSION_CURRENT	TokenNameIdentifier	 TEST  VERSION  CURRENT
,	TokenNameCOMMA	
new	TokenNamenew	
MockAnalyzer	TokenNameIdentifier	 Mock Analyzer
(	TokenNameLPAREN	
random	TokenNameIdentifier	 random
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
setOpenMode	TokenNameIdentifier	 set Open Mode
(	TokenNameLPAREN	
OpenMode	TokenNameIdentifier	 Open Mode
.	TokenNameDOT	
APPEND	TokenNameIdentifier	 APPEND
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
iw	TokenNameIdentifier	 iw
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"1 docs were added to the index, this is what we expect to find!"	TokenNameStringLiteral	1 docs were added to the index, this is what we expect to find!
,	TokenNameCOMMA	
1	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
// LUCENE-1994: test thread safety of SortableSingleDocMaker 	TokenNameCOMMENT_LINE	LUCENE-1994: test thread safety of SortableSingleDocMaker 
public	TokenNamepublic	
void	TokenNamevoid	
testDocMakerThreadSafety	TokenNameIdentifier	 test Doc Maker Thread Safety
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"log.step.AddDoc=10000"	TokenNameStringLiteral	log.step.AddDoc=10000
,	TokenNameCOMMA	
"content.source.forever=true"	TokenNameStringLiteral	content.source.forever=true
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.reuse.fields=false"	TokenNameStringLiteral	doc.reuse.fields=false
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"doc.index.props=true"	TokenNameStringLiteral	doc.index.props=true
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"[ { AddDoc > : 250 ] : 4"	TokenNameStringLiteral	[ { AddDoc > : 250 ] : 4
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. we test this value later 	TokenNameCOMMENT_LINE	2. we test this value later 
CountingSearchTestTask	TokenNameIdentifier	 Counting Search Test Task
.	TokenNameDOT	
numSearches	TokenNameIdentifier	 num Searches
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// 3. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	3. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
r	TokenNameIdentifier	 r
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
StringIndex	TokenNameIdentifier	 String Index
idx	TokenNameIdentifier	 idx
=	TokenNameEQUAL	
FieldCache	TokenNameIdentifier	 Field Cache
.	TokenNameDOT	
DEFAULT	TokenNameIdentifier	 DEFAULT
.	TokenNameDOT	
getStringIndex	TokenNameIdentifier	 get String Index
(	TokenNameLPAREN	
r	TokenNameIdentifier	 r
,	TokenNameCOMMA	
"country"	TokenNameStringLiteral	country
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
final	TokenNamefinal	
int	TokenNameint	
maxDoc	TokenNameIdentifier	 max Doc
=	TokenNameEQUAL	
r	TokenNameIdentifier	 r
.	TokenNameDOT	
maxDoc	TokenNameIdentifier	 max Doc
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
1000	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
maxDoc	TokenNameIdentifier	 max Doc
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
for	TokenNamefor	
(	TokenNameLPAREN	
int	TokenNameint	
i	TokenNameIdentifier	 i
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
i	TokenNameIdentifier	 i
<	TokenNameLESS	
1000	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
i	TokenNameIdentifier	 i
++	TokenNamePLUS_PLUS	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
assertNotNull	TokenNameIdentifier	 assert Not Null
(	TokenNameLPAREN	
"doc "	TokenNameStringLiteral	doc 
+	TokenNamePLUS	
i	TokenNameIdentifier	 i
+	TokenNamePLUS	
" has null country"	TokenNameStringLiteral	 has null country
,	TokenNameCOMMA	
idx	TokenNameIdentifier	 idx
.	TokenNameDOT	
lookup	TokenNameIdentifier	 lookup
[	TokenNameLBRACKET	
idx	TokenNameIdentifier	 idx
.	TokenNameDOT	
order	TokenNameIdentifier	 order
[	TokenNameLBRACKET	
i	TokenNameIdentifier	 i
]	TokenNameRBRACKET	
]	TokenNameRBRACKET	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
r	TokenNameIdentifier	 r
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test Parallel Doc Maker logic (for LUCENE-940) */	TokenNameCOMMENT_JAVADOC	 Test Parallel Doc Maker logic (for LUCENE-940) 
public	TokenNamepublic	
void	TokenNamevoid	
testParallelDocMaker	TokenNameIdentifier	 test Parallel Doc Maker
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=FSDirectory"	TokenNameStringLiteral	directory=FSDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"[ { AddDoc } : * ] : 4 "	TokenNameStringLiteral	[ { AddDoc } : * ] : 4 
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test WriteLineDoc and LineDocSource. */	TokenNameCOMMENT_JAVADOC	 Test WriteLineDoc and LineDocSource. 
public	TokenNamepublic	
void	TokenNamevoid	
testLineDocFile	TokenNameIdentifier	 test Line Doc File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
File	TokenNameIdentifier	 File
lineFile	TokenNameIdentifier	 line File
=	TokenNameEQUAL	
new	TokenNamenew	
File	TokenNameIdentifier	 File
(	TokenNameLPAREN	
TEMP_DIR	TokenNameIdentifier	 TEMP  DIR
,	TokenNameCOMMA	
"test.reuters.lines.txt"	TokenNameStringLiteral	test.reuters.lines.txt
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// We will call WriteLineDocs this many times 	TokenNameCOMMENT_LINE	We will call WriteLineDocs this many times 
final	TokenNamefinal	
int	TokenNameint	
NUM_TRY_DOCS	TokenNameIdentifier	 NUM  TRY  DOCS
=	TokenNameEQUAL	
50	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// Creates a line file with first 50 docs from SingleDocSource 	TokenNameCOMMENT_LINE	Creates a line file with first 50 docs from SingleDocSource 
String	TokenNameIdentifier	 String
algLines1	TokenNameIdentifier	 alg Lines1
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource
,	TokenNameCOMMA	
"content.source.forever=true"	TokenNameStringLiteral	content.source.forever=true
,	TokenNameCOMMA	
"line.file.out="	TokenNameStringLiteral	line.file.out=
+	TokenNamePLUS	
lineFile	TokenNameIdentifier	 line File
.	TokenNameDOT	
getAbsolutePath	TokenNameIdentifier	 get Absolute Path
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
replace	TokenNameIdentifier	 replace
(	TokenNameLPAREN	
'\\'	TokenNameCharacterLiteral	
,	TokenNameCOMMA	
'/'	TokenNameCharacterLiteral	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{WriteLineDoc()}:"	TokenNameStringLiteral	{WriteLineDoc()}:
+	TokenNamePLUS	
NUM_TRY_DOCS	TokenNameIdentifier	 NUM  TRY  DOCS
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// Run algo 	TokenNameCOMMENT_LINE	Run algo 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines1	TokenNameIdentifier	 alg Lines1
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
BufferedReader	TokenNameIdentifier	 Buffered Reader
r	TokenNameIdentifier	 r
=	TokenNameEQUAL	
new	TokenNamenew	
BufferedReader	TokenNameIdentifier	 Buffered Reader
(	TokenNameLPAREN	
new	TokenNamenew	
FileReader	TokenNameIdentifier	 File Reader
(	TokenNameLPAREN	
lineFile	TokenNameIdentifier	 line File
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
numLines	TokenNameIdentifier	 num Lines
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
String	TokenNameIdentifier	 String
line	TokenNameIdentifier	 line
;	TokenNameSEMICOLON	
while	TokenNamewhile	
(	TokenNameLPAREN	
(	TokenNameLPAREN	
line	TokenNameIdentifier	 line
=	TokenNameEQUAL	
r	TokenNameIdentifier	 r
.	TokenNameDOT	
readLine	TokenNameIdentifier	 read Line
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
!=	TokenNameNOT_EQUAL	
null	TokenNamenull	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
if	TokenNameif	
(	TokenNameLPAREN	
numLines	TokenNameIdentifier	 num Lines
==	TokenNameEQUAL_EQUAL	
0	TokenNameIntegerLiteral	
&&	TokenNameAND_AND	
line	TokenNameIdentifier	 line
.	TokenNameDOT	
startsWith	TokenNameIdentifier	 starts With
(	TokenNameLPAREN	
WriteLineDocTask	TokenNameIdentifier	 Write Line Doc Task
.	TokenNameDOT	
FIELDS_HEADER_INDICATOR	TokenNameIdentifier	 FIELDS  HEADER  INDICATOR
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
continue	TokenNamecontinue	
;	TokenNameSEMICOLON	
// do not count the header line as a doc 	TokenNameCOMMENT_LINE	do not count the header line as a doc 
}	TokenNameRBRACE	
numLines	TokenNameIdentifier	 num Lines
++	TokenNamePLUS_PLUS	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
r	TokenNameIdentifier	 r
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"did not see the right number of docs; should be "	TokenNameStringLiteral	did not see the right number of docs; should be 
+	TokenNamePLUS	
NUM_TRY_DOCS	TokenNameIdentifier	 NUM  TRY  DOCS
+	TokenNamePLUS	
" but was "	TokenNameStringLiteral	 but was 
+	TokenNamePLUS	
numLines	TokenNameIdentifier	 num Lines
,	TokenNameCOMMA	
NUM_TRY_DOCS	TokenNameIdentifier	 NUM  TRY  DOCS
,	TokenNameCOMMA	
numLines	TokenNameIdentifier	 num Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// Index the line docs 	TokenNameCOMMENT_LINE	Index the line docs 
String	TokenNameIdentifier	 String
algLines2	TokenNameIdentifier	 alg Lines2
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer"	TokenNameStringLiteral	analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
lineFile	TokenNameIdentifier	 line File
.	TokenNameDOT	
getAbsolutePath	TokenNameIdentifier	 get Absolute Path
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
replace	TokenNameIdentifier	 replace
(	TokenNameLPAREN	
'\\'	TokenNameCharacterLiteral	
,	TokenNameCOMMA	
'/'	TokenNameCharacterLiteral	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"doc.reuse.fields=false"	TokenNameStringLiteral	doc.reuse.fields=false
,	TokenNameCOMMA	
"ram.flush.mb=4"	TokenNameStringLiteral	ram.flush.mb=4
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{AddDoc}: *"	TokenNameStringLiteral	{AddDoc}: *
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// Run algo 	TokenNameCOMMENT_LINE	Run algo 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines2	TokenNameIdentifier	 alg Lines2
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// now we should be able to open the index for write. 	TokenNameCOMMENT_LINE	now we should be able to open the index for write. 
IndexWriter	TokenNameIdentifier	 Index Writer
iw	TokenNameIdentifier	 iw
=	TokenNameEQUAL	
new	TokenNamenew	
IndexWriter	TokenNameIdentifier	 Index Writer
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
new	TokenNamenew	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
(	TokenNameLPAREN	
TEST_VERSION_CURRENT	TokenNameIdentifier	 TEST  VERSION  CURRENT
,	TokenNameCOMMA	
new	TokenNamenew	
MockAnalyzer	TokenNameIdentifier	 Mock Analyzer
(	TokenNameLPAREN	
random	TokenNameIdentifier	 random
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
setOpenMode	TokenNameIdentifier	 set Open Mode
(	TokenNameLPAREN	
OpenMode	TokenNameIdentifier	 Open Mode
.	TokenNameDOT	
APPEND	TokenNameIdentifier	 APPEND
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
iw	TokenNameIdentifier	 iw
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
numLines	TokenNameIdentifier	 num Lines
+	TokenNamePLUS	
" lines were created but "	TokenNameStringLiteral	 lines were created but 
+	TokenNamePLUS	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
+	TokenNamePLUS	
" docs are in the index"	TokenNameStringLiteral	 docs are in the index
,	TokenNameCOMMA	
numLines	TokenNameIdentifier	 num Lines
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
lineFile	TokenNameIdentifier	 line File
.	TokenNameDOT	
delete	TokenNameIdentifier	 delete
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test ReadTokensTask */	TokenNameCOMMENT_JAVADOC	 Test ReadTokensTask 
public	TokenNamepublic	
void	TokenNamevoid	
testReadTokens	TokenNameIdentifier	 test Read Tokens
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// We will call ReadTokens on this many docs 	TokenNameCOMMENT_LINE	We will call ReadTokens on this many docs 
final	TokenNamefinal	
int	TokenNameint	
NUM_DOCS	TokenNameIdentifier	 NUM  DOCS
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// Read tokens from first NUM_DOCS docs from Reuters and 	TokenNameCOMMENT_LINE	Read tokens from first NUM_DOCS docs from Reuters and 
// then build index from the same docs 	TokenNameCOMMENT_LINE	then build index from the same docs 
String	TokenNameIdentifier	 String
algLines1	TokenNameIdentifier	 alg Lines1
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer"	TokenNameStringLiteral	analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ReadTokens}: "	TokenNameStringLiteral	{ReadTokens}: 
+	TokenNamePLUS	
NUM_DOCS	TokenNameIdentifier	 NUM  DOCS
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{AddDoc}: "	TokenNameStringLiteral	{AddDoc}: 
+	TokenNamePLUS	
NUM_DOCS	TokenNameIdentifier	 NUM  DOCS
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// Run algo 	TokenNameCOMMENT_LINE	Run algo 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines1	TokenNameIdentifier	 alg Lines1
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
List	TokenNameIdentifier	 List
<	TokenNameLESS	
TaskStats	TokenNameIdentifier	 Task Stats
>	TokenNameGREATER	
stats	TokenNameIdentifier	 stats
=	TokenNameEQUAL	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getPoints	TokenNameIdentifier	 get Points
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
taskStats	TokenNameIdentifier	 task Stats
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// Count how many tokens all ReadTokens saw 	TokenNameCOMMENT_LINE	Count how many tokens all ReadTokens saw 
int	TokenNameint	
totalTokenCount1	TokenNameIdentifier	 total Token Count1
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
for	TokenNamefor	
(	TokenNameLPAREN	
final	TokenNamefinal	
TaskStats	TokenNameIdentifier	 Task Stats
stat	TokenNameIdentifier	 stat
:	TokenNameCOLON	
stats	TokenNameIdentifier	 stats
)	TokenNameRPAREN	
{	TokenNameLBRACE	
if	TokenNameif	
(	TokenNameLPAREN	
stat	TokenNameIdentifier	 stat
.	TokenNameDOT	
getTask	TokenNameIdentifier	 get Task
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
equals	TokenNameIdentifier	 equals
(	TokenNameLPAREN	
"ReadTokens"	TokenNameStringLiteral	ReadTokens
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
totalTokenCount1	TokenNameIdentifier	 total Token Count1
+=	TokenNamePLUS_EQUAL	
stat	TokenNameIdentifier	 stat
.	TokenNameDOT	
getCount	TokenNameIdentifier	 get Count
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
// Separately count how many tokens are actually in the index: 	TokenNameCOMMENT_LINE	Separately count how many tokens are actually in the index: 
IndexReader	TokenNameIdentifier	 Index Reader
reader	TokenNameIdentifier	 reader
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
NUM_DOCS	TokenNameIdentifier	 NUM  DOCS
,	TokenNameCOMMA	
reader	TokenNameIdentifier	 reader
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
TermEnum	TokenNameIdentifier	 Term Enum
terms	TokenNameIdentifier	 terms
=	TokenNameEQUAL	
reader	TokenNameIdentifier	 reader
.	TokenNameDOT	
terms	TokenNameIdentifier	 terms
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
TermDocs	TokenNameIdentifier	 Term Docs
termDocs	TokenNameIdentifier	 term Docs
=	TokenNameEQUAL	
reader	TokenNameIdentifier	 reader
.	TokenNameDOT	
termDocs	TokenNameIdentifier	 term Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
totalTokenCount2	TokenNameIdentifier	 total Token Count2
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
while	TokenNamewhile	
(	TokenNameLPAREN	
terms	TokenNameIdentifier	 terms
.	TokenNameDOT	
next	TokenNameIdentifier	 next
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
Term	TokenNameIdentifier	 Term
term	TokenNameIdentifier	 term
=	TokenNameEQUAL	
terms	TokenNameIdentifier	 terms
.	TokenNameDOT	
term	TokenNameIdentifier	 term
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
/* not-tokenized, but indexed field */	TokenNameCOMMENT_BLOCK	 not-tokenized, but indexed field 
if	TokenNameif	
(	TokenNameLPAREN	
term	TokenNameIdentifier	 term
!=	TokenNameNOT_EQUAL	
null	TokenNamenull	
&&	TokenNameAND_AND	
term	TokenNameIdentifier	 term
.	TokenNameDOT	
field	TokenNameIdentifier	 field
(	TokenNameLPAREN	
)	TokenNameRPAREN	
!=	TokenNameNOT_EQUAL	
DocMaker	TokenNameIdentifier	 Doc Maker
.	TokenNameDOT	
ID_FIELD	TokenNameIdentifier	 ID  FIELD
&&	TokenNameAND_AND	
term	TokenNameIdentifier	 term
.	TokenNameDOT	
field	TokenNameIdentifier	 field
(	TokenNameLPAREN	
)	TokenNameRPAREN	
!=	TokenNameNOT_EQUAL	
DocMaker	TokenNameIdentifier	 Doc Maker
.	TokenNameDOT	
DATE_MSEC_FIELD	TokenNameIdentifier	 DATE  MSEC  FIELD
&&	TokenNameAND_AND	
term	TokenNameIdentifier	 term
.	TokenNameDOT	
field	TokenNameIdentifier	 field
(	TokenNameLPAREN	
)	TokenNameRPAREN	
!=	TokenNameNOT_EQUAL	
DocMaker	TokenNameIdentifier	 Doc Maker
.	TokenNameDOT	
TIME_SEC_FIELD	TokenNameIdentifier	 TIME  SEC  FIELD
)	TokenNameRPAREN	
{	TokenNameLBRACE	
termDocs	TokenNameIdentifier	 term Docs
.	TokenNameDOT	
seek	TokenNameIdentifier	 seek
(	TokenNameLPAREN	
terms	TokenNameIdentifier	 terms
.	TokenNameDOT	
term	TokenNameIdentifier	 term
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
while	TokenNamewhile	
(	TokenNameLPAREN	
termDocs	TokenNameIdentifier	 term Docs
.	TokenNameDOT	
next	TokenNameIdentifier	 next
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
totalTokenCount2	TokenNameIdentifier	 total Token Count2
+=	TokenNamePLUS_EQUAL	
termDocs	TokenNameIdentifier	 term Docs
.	TokenNameDOT	
freq	TokenNameIdentifier	 freq
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
reader	TokenNameIdentifier	 reader
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// Make sure they are the same 	TokenNameCOMMENT_LINE	Make sure they are the same 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
totalTokenCount1	TokenNameIdentifier	 total Token Count1
,	TokenNameCOMMA	
totalTokenCount2	TokenNameIdentifier	 total Token Count2
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that " {[AddDoc(4000)]: 4} : * " works corrcetly (for LUCENE-941) */	TokenNameCOMMENT_JAVADOC	 Test that " {[AddDoc(4000)]: 4} : " works corrcetly (for LUCENE-941) 
public	TokenNamepublic	
void	TokenNamevoid	
testParallelExhausted	TokenNameIdentifier	 test Parallel Exhausted
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"task.max.depth.log=1"	TokenNameStringLiteral	task.max.depth.log=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ [ AddDoc]: 4} : * "	TokenNameStringLiteral	{ [ AddDoc]: 4} : * 
,	TokenNameCOMMA	
"ResetInputs "	TokenNameStringLiteral	ResetInputs 
,	TokenNameCOMMA	
"{ [ AddDoc]: 4} : * "	TokenNameStringLiteral	{ [ AddDoc]: 4} : * 
,	TokenNameCOMMA	
"WaitForMerges"	TokenNameStringLiteral	WaitForMerges
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
2	TokenNameIntegerLiteral	
*	TokenNameMULTIPLY	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that exhaust in loop works as expected (LUCENE-1115). */	TokenNameCOMMENT_JAVADOC	 Test that exhaust in loop works as expected (LUCENE-1115). 
public	TokenNamepublic	
void	TokenNamevoid	
testExhaustedLooped	TokenNameIdentifier	 test Exhausted Looped
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"task.max.depth.log=1"	TokenNameStringLiteral	task.max.depth.log=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" WaitForMerges"	TokenNameStringLiteral	 WaitForMerges
,	TokenNameCOMMA	
" CloseIndex"	TokenNameStringLiteral	 CloseIndex
,	TokenNameCOMMA	
"} : 2"	TokenNameStringLiteral	} : 2
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that we can close IndexWriter with argument "false". */	TokenNameCOMMENT_JAVADOC	 Test that we can close IndexWriter with argument "false". 
public	TokenNamepublic	
void	TokenNamevoid	
testCloseIndexFalse	TokenNameIdentifier	 test Close Index False
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"ram.flush.mb=-1"	TokenNameStringLiteral	ram.flush.mb=-1
,	TokenNameCOMMA	
"max.buffered=2"	TokenNameStringLiteral	max.buffered=2
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" CloseIndex(false)"	TokenNameStringLiteral	 CloseIndex(false)
,	TokenNameCOMMA	
"} : 2"	TokenNameStringLiteral	} : 2
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
public	TokenNamepublic	
static	TokenNamestatic	
class	TokenNameclass	
MyMergeScheduler	TokenNameIdentifier	 My Merge Scheduler
extends	TokenNameextends	
SerialMergeScheduler	TokenNameIdentifier	 Serial Merge Scheduler
{	TokenNameLBRACE	
boolean	TokenNameboolean	
called	TokenNameIdentifier	 called
;	TokenNameSEMICOLON	
public	TokenNamepublic	
MyMergeScheduler	TokenNameIdentifier	 My Merge Scheduler
(	TokenNameLPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
super	TokenNamesuper	
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
called	TokenNameIdentifier	 called
=	TokenNameEQUAL	
true	TokenNametrue	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
public	TokenNamepublic	
void	TokenNamevoid	
testDeleteByPercent	TokenNameIdentifier	 test Delete By Percent
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"ram.flush.mb=-1"	TokenNameStringLiteral	ram.flush.mb=-1
,	TokenNameCOMMA	
"max.buffered=2"	TokenNameStringLiteral	max.buffered=2
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ "AddDocs" AddDoc > : * "	TokenNameStringLiteral	{ "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
"CloseIndex()"	TokenNameStringLiteral	CloseIndex()
,	TokenNameCOMMA	
"OpenReader(false)"	TokenNameStringLiteral	OpenReader(false)
,	TokenNameCOMMA	
"DeleteByPercent(20)"	TokenNameStringLiteral	DeleteByPercent(20)
,	TokenNameCOMMA	
"CloseReader"	TokenNameStringLiteral	CloseReader
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
16	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs, minus 20% 	TokenNameCOMMENT_LINE	first 20 reuters docs, minus 20% 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that we can set merge scheduler". */	TokenNameCOMMENT_JAVADOC	 Test that we can set merge scheduler". 
public	TokenNamepublic	
void	TokenNamevoid	
testMergeScheduler	TokenNameIdentifier	 test Merge Scheduler
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"merge.scheduler="	TokenNameStringLiteral	merge.scheduler=
+	TokenNamePLUS	
MyMergeScheduler	TokenNameIdentifier	 My Merge Scheduler
.	TokenNameDOT	
class	TokenNameclass	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
"} : 2"	TokenNameStringLiteral	} : 2
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"did not use the specified MergeScheduler"	TokenNameStringLiteral	did not use the specified MergeScheduler
,	TokenNameCOMMA	
(	TokenNameLPAREN	
(	TokenNameLPAREN	
MyMergeScheduler	TokenNameIdentifier	 My Merge Scheduler
)	TokenNameRPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getIndexWriter	TokenNameIdentifier	 get Index Writer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getConfig	TokenNameIdentifier	 get Config
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getMergeScheduler	TokenNameIdentifier	 get Merge Scheduler
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
called	TokenNameIdentifier	 called
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getIndexWriter	TokenNameIdentifier	 get Index Writer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
public	TokenNamepublic	
static	TokenNamestatic	
class	TokenNameclass	
MyMergePolicy	TokenNameIdentifier	 My Merge Policy
extends	TokenNameextends	
LogDocMergePolicy	TokenNameIdentifier	 Log Doc Merge Policy
{	TokenNameLBRACE	
boolean	TokenNameboolean	
called	TokenNameIdentifier	 called
;	TokenNameSEMICOLON	
public	TokenNamepublic	
MyMergePolicy	TokenNameIdentifier	 My Merge Policy
(	TokenNameLPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
called	TokenNameIdentifier	 called
=	TokenNameEQUAL	
true	TokenNametrue	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
/** * Test that we can set merge policy". */	TokenNameCOMMENT_JAVADOC	 Test that we can set merge policy". 
public	TokenNamepublic	
void	TokenNamevoid	
testMergePolicy	TokenNameIdentifier	 test Merge Policy
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"ram.flush.mb=-1"	TokenNameStringLiteral	ram.flush.mb=-1
,	TokenNameCOMMA	
"max.buffered=2"	TokenNameStringLiteral	max.buffered=2
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"merge.policy="	TokenNameStringLiteral	merge.policy=
+	TokenNamePLUS	
MyMergePolicy	TokenNameIdentifier	 My Merge Policy
.	TokenNameDOT	
class	TokenNameclass	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
"} : 2"	TokenNameStringLiteral	} : 2
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"did not use the specified MergePolicy"	TokenNameStringLiteral	did not use the specified MergePolicy
,	TokenNameCOMMA	
(	TokenNameLPAREN	
(	TokenNameLPAREN	
MyMergePolicy	TokenNameIdentifier	 My Merge Policy
)	TokenNameRPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getIndexWriter	TokenNameIdentifier	 get Index Writer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getConfig	TokenNameIdentifier	 get Config
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getMergePolicy	TokenNameIdentifier	 get Merge Policy
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
called	TokenNameIdentifier	 called
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getIndexWriter	TokenNameIdentifier	 get Index Writer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that IndexWriter settings stick. */	TokenNameCOMMENT_JAVADOC	 Test that IndexWriter settings stick. 
public	TokenNamepublic	
void	TokenNamevoid	
testIndexWriterSettings	TokenNameIdentifier	 test Index Writer Settings
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"ram.flush.mb=-1"	TokenNameStringLiteral	ram.flush.mb=-1
,	TokenNameCOMMA	
"max.buffered=2"	TokenNameStringLiteral	max.buffered=2
,	TokenNameCOMMA	
"compound=cmpnd:true:false"	TokenNameStringLiteral	compound=cmpnd:true:false
,	TokenNameCOMMA	
"doc.term.vector=vector:false:true"	TokenNameStringLiteral	doc.term.vector=vector:false:true
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"merge.factor=3"	TokenNameStringLiteral	merge.factor=3
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" NewRound"	TokenNameStringLiteral	 NewRound
,	TokenNameCOMMA	
"} : 2"	TokenNameStringLiteral	} : 2
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
final	TokenNamefinal	
IndexWriter	TokenNameIdentifier	 Index Writer
writer	TokenNameIdentifier	 writer
=	TokenNameEQUAL	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getIndexWriter	TokenNameIdentifier	 get Index Writer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
2	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
writer	TokenNameIdentifier	 writer
.	TokenNameDOT	
getConfig	TokenNameIdentifier	 get Config
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getMaxBufferedDocs	TokenNameIdentifier	 get Max Buffered Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
IndexWriterConfig	TokenNameIdentifier	 Index Writer Config
.	TokenNameDOT	
DISABLE_AUTO_FLUSH	TokenNameIdentifier	 DISABLE  AUTO  FLUSH
,	TokenNameCOMMA	
(	TokenNameLPAREN	
int	TokenNameint	
)	TokenNameRPAREN	
writer	TokenNameIdentifier	 writer
.	TokenNameDOT	
getConfig	TokenNameIdentifier	 get Config
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getRAMBufferSizeMB	TokenNameIdentifier	 get RAM Buffer Size MB
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
3	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
(	TokenNameLPAREN	
(	TokenNameLPAREN	
LogMergePolicy	TokenNameIdentifier	 Log Merge Policy
)	TokenNameRPAREN	
writer	TokenNameIdentifier	 writer
.	TokenNameDOT	
getConfig	TokenNameIdentifier	 get Config
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getMergePolicy	TokenNameIdentifier	 get Merge Policy
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getMergeFactor	TokenNameIdentifier	 get Merge Factor
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertFalse	TokenNameIdentifier	 assert False
(	TokenNameLPAREN	
(	TokenNameLPAREN	
(	TokenNameLPAREN	
LogMergePolicy	TokenNameIdentifier	 Log Merge Policy
)	TokenNameRPAREN	
writer	TokenNameIdentifier	 writer
.	TokenNameDOT	
getConfig	TokenNameIdentifier	 get Config
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getMergePolicy	TokenNameIdentifier	 get Merge Policy
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getUseCompoundFile	TokenNameIdentifier	 get Use Compound File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
writer	TokenNameIdentifier	 writer
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
Directory	TokenNameIdentifier	 Directory
dir	TokenNameIdentifier	 dir
=	TokenNameEQUAL	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
IndexReader	TokenNameIdentifier	 Index Reader
reader	TokenNameIdentifier	 reader
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
dir	TokenNameIdentifier	 dir
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
TermFreqVector	TokenNameIdentifier	 Term Freq Vector
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
tfv	TokenNameIdentifier	 tfv
=	TokenNameEQUAL	
reader	TokenNameIdentifier	 reader
.	TokenNameDOT	
getTermFreqVectors	TokenNameIdentifier	 get Term Freq Vectors
(	TokenNameLPAREN	
0	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertNotNull	TokenNameIdentifier	 assert Not Null
(	TokenNameLPAREN	
tfv	TokenNameIdentifier	 tfv
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
tfv	TokenNameIdentifier	 tfv
.	TokenNameDOT	
length	TokenNameIdentifier	 length
>	TokenNameGREATER	
0	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
reader	TokenNameIdentifier	 reader
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test indexing with facets tasks. */	TokenNameCOMMENT_JAVADOC	 Test indexing with facets tasks. 
public	TokenNamepublic	
void	TokenNamevoid	
testIndexingWithFacets	TokenNameIdentifier	 test Indexing With Facets
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=100"	TokenNameStringLiteral	content.source.log.step=100
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"merge.factor=3"	TokenNameStringLiteral	merge.factor=3
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"ResetSystemErase"	TokenNameStringLiteral	ResetSystemErase
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"CreateTaxonomyIndex"	TokenNameStringLiteral	CreateTaxonomyIndex
,	TokenNameCOMMA	
"{ "AddDocs" AddFacetedDoc > : * "	TokenNameStringLiteral	{ "AddDocs" AddFacetedDoc > : * 
,	TokenNameCOMMA	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
,	TokenNameCOMMA	
"CloseTaxonomyIndex"	TokenNameStringLiteral	CloseTaxonomyIndex
,	TokenNameCOMMA	
"OpenTaxonomyReader"	TokenNameStringLiteral	OpenTaxonomyReader
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
PerfRunData	TokenNameIdentifier	 Perf Run Data
runData	TokenNameIdentifier	 run Data
=	TokenNameEQUAL	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertNull	TokenNameIdentifier	 assert Null
(	TokenNameLPAREN	
"taxo writer was not properly closed"	TokenNameStringLiteral	taxo writer was not properly closed
,	TokenNameCOMMA	
runData	TokenNameIdentifier	 run Data
.	TokenNameDOT	
getTaxonomyWriter	TokenNameIdentifier	 get Taxonomy Writer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
TaxonomyReader	TokenNameIdentifier	 Taxonomy Reader
taxoReader	TokenNameIdentifier	 taxo Reader
=	TokenNameEQUAL	
runData	TokenNameIdentifier	 run Data
.	TokenNameDOT	
getTaxonomyReader	TokenNameIdentifier	 get Taxonomy Reader
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertNotNull	TokenNameIdentifier	 assert Not Null
(	TokenNameLPAREN	
"taxo reader was not opened"	TokenNameStringLiteral	taxo reader was not opened
,	TokenNameCOMMA	
taxoReader	TokenNameIdentifier	 taxo Reader
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
"nothing was added to the taxnomy (expecting root and at least one addtional category)"	TokenNameStringLiteral	nothing was added to the taxnomy (expecting root and at least one addtional category)
,	TokenNameCOMMA	
taxoReader	TokenNameIdentifier	 taxo Reader
.	TokenNameDOT	
getSize	TokenNameIdentifier	 get Size
(	TokenNameLPAREN	
)	TokenNameRPAREN	
>	TokenNameGREATER	
1	TokenNameIntegerLiteral	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
taxoReader	TokenNameIdentifier	 taxo Reader
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that we can call forceMerge(maxNumSegments). */	TokenNameCOMMENT_JAVADOC	 Test that we can call forceMerge(maxNumSegments). 
public	TokenNamepublic	
void	TokenNamevoid	
testForceMerge	TokenNameIdentifier	 test Force Merge
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"ram.flush.mb=-1"	TokenNameStringLiteral	ram.flush.mb=-1
,	TokenNameCOMMA	
"max.buffered=3"	TokenNameStringLiteral	max.buffered=3
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"merge.policy=org.apache.lucene.index.LogDocMergePolicy"	TokenNameStringLiteral	merge.policy=org.apache.lucene.index.LogDocMergePolicy
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"debug.level=1"	TokenNameStringLiteral	debug.level=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" ForceMerge(3)"	TokenNameStringLiteral	 ForceMerge(3)
,	TokenNameCOMMA	
" CloseIndex()"	TokenNameStringLiteral	 CloseIndex()
,	TokenNameCOMMA	
"} : 2"	TokenNameStringLiteral	} : 2
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test number of docs in the index 	TokenNameCOMMENT_LINE	3. test number of docs in the index 
IndexReader	TokenNameIdentifier	 Index Reader
ir	TokenNameIdentifier	 ir
=	TokenNameEQUAL	
IndexReader	TokenNameIdentifier	 Index Reader
.	TokenNameDOT	
open	TokenNameIdentifier	 open
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
int	TokenNameint	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
=	TokenNameEQUAL	
20	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
// first 20 reuters docs. 	TokenNameCOMMENT_LINE	first 20 reuters docs. 
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"wrong number of docs in the index!"	TokenNameStringLiteral	wrong number of docs in the index!
,	TokenNameCOMMA	
ndocsExpected	TokenNameIdentifier	 ndocs Expected
,	TokenNameCOMMA	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
numDocs	TokenNameIdentifier	 num Docs
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ir	TokenNameIdentifier	 ir
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// Make sure we have 3 segments: 	TokenNameCOMMENT_LINE	Make sure we have 3 segments: 
SegmentInfos	TokenNameIdentifier	 Segment Infos
infos	TokenNameIdentifier	 infos
=	TokenNameEQUAL	
new	TokenNamenew	
SegmentInfos	TokenNameIdentifier	 Segment Infos
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
infos	TokenNameIdentifier	 infos
.	TokenNameDOT	
read	TokenNameIdentifier	 read
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getDirectory	TokenNameIdentifier	 get Directory
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
3	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
infos	TokenNameIdentifier	 infos
.	TokenNameDOT	
size	TokenNameIdentifier	 size
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test disabling task count (LUCENE-1136). */	TokenNameCOMMENT_JAVADOC	 Test disabling task count (LUCENE-1136). 
public	TokenNamepublic	
void	TokenNamevoid	
testDisableCounting	TokenNameIdentifier	 test Disable Counting
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
doTestDisableCounting	TokenNameIdentifier	 do Test Disable Counting
(	TokenNameLPAREN	
true	TokenNametrue	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
doTestDisableCounting	TokenNameIdentifier	 do Test Disable Counting
(	TokenNameLPAREN	
false	TokenNamefalse	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
void	TokenNamevoid	
doTestDisableCounting	TokenNameIdentifier	 do Test Disable Counting
(	TokenNameLPAREN	
boolean	TokenNameboolean	
disable	TokenNameIdentifier	 disable
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// 1. alg definition (required in every "logic" test) 	TokenNameCOMMENT_LINE	1. alg definition (required in every "logic" test) 
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
disableCountingLines	TokenNameIdentifier	 disable Counting Lines
(	TokenNameLPAREN	
disable	TokenNameIdentifier	 disable
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 2. execute the algorithm (required in every "logic" test) 	TokenNameCOMMENT_LINE	2. execute the algorithm (required in every "logic" test) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
algLines	TokenNameIdentifier	 alg Lines
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// 3. test counters 	TokenNameCOMMENT_LINE	3. test counters 
int	TokenNameint	
n	TokenNameIdentifier	 n
=	TokenNameEQUAL	
disable	TokenNameIdentifier	 disable
?	TokenNameQUESTION	
0	TokenNameIntegerLiteral	
:	TokenNameCOLON	
1	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
int	TokenNameint	
nChecked	TokenNameIdentifier	 n Checked
=	TokenNameEQUAL	
0	TokenNameIntegerLiteral	
;	TokenNameSEMICOLON	
for	TokenNamefor	
(	TokenNameLPAREN	
final	TokenNamefinal	
TaskStats	TokenNameIdentifier	 Task Stats
stats	TokenNameIdentifier	 stats
:	TokenNameCOLON	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getPoints	TokenNameIdentifier	 get Points
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
taskStats	TokenNameIdentifier	 task Stats
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
taskName	TokenNameIdentifier	 task Name
=	TokenNameEQUAL	
stats	TokenNameIdentifier	 stats
.	TokenNameDOT	
getTask	TokenNameIdentifier	 get Task
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getName	TokenNameIdentifier	 get Name
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
if	TokenNameif	
(	TokenNameLPAREN	
taskName	TokenNameIdentifier	 task Name
.	TokenNameDOT	
equals	TokenNameIdentifier	 equals
(	TokenNameLPAREN	
"Rounds"	TokenNameStringLiteral	Rounds
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"Wrong total count!"	TokenNameStringLiteral	Wrong total count!
,	TokenNameCOMMA	
20	TokenNameIntegerLiteral	
+	TokenNamePLUS	
2	TokenNameIntegerLiteral	
*	TokenNameMULTIPLY	
n	TokenNameIdentifier	 n
,	TokenNameCOMMA	
stats	TokenNameIdentifier	 stats
.	TokenNameDOT	
getCount	TokenNameIdentifier	 get Count
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
nChecked	TokenNameIdentifier	 n Checked
++	TokenNamePLUS_PLUS	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
else	TokenNameelse	
if	TokenNameif	
(	TokenNameLPAREN	
taskName	TokenNameIdentifier	 task Name
.	TokenNameDOT	
equals	TokenNameIdentifier	 equals
(	TokenNameLPAREN	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"Wrong count for CreateIndex!"	TokenNameStringLiteral	Wrong count for CreateIndex!
,	TokenNameCOMMA	
n	TokenNameIdentifier	 n
,	TokenNameCOMMA	
stats	TokenNameIdentifier	 stats
.	TokenNameDOT	
getCount	TokenNameIdentifier	 get Count
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
nChecked	TokenNameIdentifier	 n Checked
++	TokenNamePLUS_PLUS	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
else	TokenNameelse	
if	TokenNameif	
(	TokenNameLPAREN	
taskName	TokenNameIdentifier	 task Name
.	TokenNameDOT	
equals	TokenNameIdentifier	 equals
(	TokenNameLPAREN	
"CloseIndex"	TokenNameStringLiteral	CloseIndex
)	TokenNameRPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"Wrong count for CloseIndex!"	TokenNameStringLiteral	Wrong count for CloseIndex!
,	TokenNameCOMMA	
n	TokenNameIdentifier	 n
,	TokenNameCOMMA	
stats	TokenNameIdentifier	 stats
.	TokenNameDOT	
getCount	TokenNameIdentifier	 get Count
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
nChecked	TokenNameIdentifier	 n Checked
++	TokenNamePLUS_PLUS	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
"Missing some tasks to check!"	TokenNameStringLiteral	Missing some tasks to check!
,	TokenNameCOMMA	
3	TokenNameIntegerLiteral	
,	TokenNameCOMMA	
nChecked	TokenNameIdentifier	 n Checked
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
disableCountingLines	TokenNameIdentifier	 disable Counting Lines
(	TokenNameLPAREN	
boolean	TokenNameboolean	
disable	TokenNameIdentifier	 disable
)	TokenNameRPAREN	
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
dis	TokenNameIdentifier	 dis
=	TokenNameEQUAL	
disable	TokenNameIdentifier	 disable
?	TokenNameQUESTION	
"-"	TokenNameStringLiteral	-
:	TokenNameCOLON	
""	TokenNameStringLiteral	 
;	TokenNameSEMICOLON	
return	TokenNamereturn	
new	TokenNamenew	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=30"	TokenNameStringLiteral	content.source.log.step=30
,	TokenNameCOMMA	
"doc.term.vector=false"	TokenNameStringLiteral	doc.term.vector=false
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"doc.stored=false"	TokenNameStringLiteral	doc.stored=false
,	TokenNameCOMMA	
"doc.tokenized=false"	TokenNameStringLiteral	doc.tokenized=false
,	TokenNameCOMMA	
"task.max.depth.log=1"	TokenNameStringLiteral	task.max.depth.log=1
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" "	TokenNameStringLiteral	 
+	TokenNamePLUS	
dis	TokenNameIdentifier	 dis
+	TokenNamePLUS	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
// optionally disable counting here 	TokenNameCOMMENT_LINE	optionally disable counting here 
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" "	TokenNameStringLiteral	 
+	TokenNamePLUS	
dis	TokenNameIdentifier	 dis
+	TokenNamePLUS	
" CloseIndex"	TokenNameStringLiteral	 CloseIndex
,	TokenNameCOMMA	
// optionally disable counting here (with extra blanks) 	TokenNameCOMMENT_LINE	optionally disable counting here (with extra blanks) 
"}"	TokenNameStringLiteral	}
,	TokenNameCOMMA	
"RepSumByName"	TokenNameStringLiteral	RepSumByName
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that we can change the Locale in the runData, * that it is parsed as we expect. */	TokenNameCOMMENT_JAVADOC	 Test that we can change the Locale in the runData, that it is parsed as we expect. 
public	TokenNamepublic	
void	TokenNamevoid	
testLocale	TokenNameIdentifier	 test Locale
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// empty Locale: clear it (null) 	TokenNameCOMMENT_LINE	empty Locale: clear it (null) 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getLocaleConfig	TokenNameIdentifier	 get Locale Config
(	TokenNameLPAREN	
""	TokenNameStringLiteral	 
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertNull	TokenNameIdentifier	 assert Null
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getLocale	TokenNameIdentifier	 get Locale
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// ROOT locale 	TokenNameCOMMENT_LINE	ROOT locale 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getLocaleConfig	TokenNameIdentifier	 get Locale Config
(	TokenNameLPAREN	
"ROOT"	TokenNameStringLiteral	ROOT
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
""	TokenNameStringLiteral	 
)	TokenNameRPAREN	
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getLocale	TokenNameIdentifier	 get Locale
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// specify just a language 	TokenNameCOMMENT_LINE	specify just a language 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getLocaleConfig	TokenNameIdentifier	 get Locale Config
(	TokenNameLPAREN	
"de"	TokenNameStringLiteral	de
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
"de"	TokenNameStringLiteral	de
)	TokenNameRPAREN	
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getLocale	TokenNameIdentifier	 get Locale
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// specify language + country 	TokenNameCOMMENT_LINE	specify language + country 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getLocaleConfig	TokenNameIdentifier	 get Locale Config
(	TokenNameLPAREN	
"en,US"	TokenNameStringLiteral	en,US
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
"en"	TokenNameStringLiteral	en
,	TokenNameCOMMA	
"US"	TokenNameStringLiteral	US
)	TokenNameRPAREN	
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getLocale	TokenNameIdentifier	 get Locale
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// specify language + country + variant 	TokenNameCOMMENT_LINE	specify language + country + variant 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getLocaleConfig	TokenNameIdentifier	 get Locale Config
(	TokenNameLPAREN	
"no,NO,NY"	TokenNameStringLiteral	no,NO,NY
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
"no"	TokenNameStringLiteral	no
,	TokenNameCOMMA	
"NO"	TokenNameStringLiteral	NO
,	TokenNameCOMMA	
"NY"	TokenNameStringLiteral	NY
)	TokenNameRPAREN	
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getLocale	TokenNameIdentifier	 get Locale
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
getLocaleConfig	TokenNameIdentifier	 get Locale Config
(	TokenNameLPAREN	
String	TokenNameIdentifier	 String
localeParam	TokenNameIdentifier	 locale Param
)	TokenNameRPAREN	
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" NewLocale("	TokenNameStringLiteral	 NewLocale(
+	TokenNamePLUS	
localeParam	TokenNameIdentifier	 locale Param
+	TokenNamePLUS	
")"	TokenNameStringLiteral	)
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" NewRound"	TokenNameStringLiteral	 NewRound
,	TokenNameCOMMA	
"} : 1"	TokenNameStringLiteral	} : 1
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
return	TokenNamereturn	
algLines	TokenNameIdentifier	 alg Lines
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that we can create CollationAnalyzers. */	TokenNameCOMMENT_JAVADOC	 Test that we can create CollationAnalyzers. 
public	TokenNamepublic	
void	TokenNamevoid	
testCollator	TokenNameIdentifier	 test Collator
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
// ROOT locale 	TokenNameCOMMENT_LINE	ROOT locale 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getCollatorConfig	TokenNameIdentifier	 get Collator Config
(	TokenNameLPAREN	
"ROOT"	TokenNameStringLiteral	ROOT
,	TokenNameCOMMA	
"impl:jdk"	TokenNameStringLiteral	impl:jdk
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
CollationKeyAnalyzer	TokenNameIdentifier	 Collation Key Analyzer
expected	TokenNameIdentifier	 expected
=	TokenNameEQUAL	
new	TokenNamenew	
CollationKeyAnalyzer	TokenNameIdentifier	 Collation Key Analyzer
(	TokenNameLPAREN	
Collator	TokenNameIdentifier	 Collator
.	TokenNameDOT	
getInstance	TokenNameIdentifier	 get Instance
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
""	TokenNameStringLiteral	 
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualCollation	TokenNameIdentifier	 assert Equal Collation
(	TokenNameLPAREN	
expected	TokenNameIdentifier	 expected
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"foobar"	TokenNameStringLiteral	foobar
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// specify just a language 	TokenNameCOMMENT_LINE	specify just a language 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getCollatorConfig	TokenNameIdentifier	 get Collator Config
(	TokenNameLPAREN	
"de"	TokenNameStringLiteral	de
,	TokenNameCOMMA	
"impl:jdk"	TokenNameStringLiteral	impl:jdk
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
expected	TokenNameIdentifier	 expected
=	TokenNameEQUAL	
new	TokenNamenew	
CollationKeyAnalyzer	TokenNameIdentifier	 Collation Key Analyzer
(	TokenNameLPAREN	
Collator	TokenNameIdentifier	 Collator
.	TokenNameDOT	
getInstance	TokenNameIdentifier	 get Instance
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
"de"	TokenNameStringLiteral	de
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualCollation	TokenNameIdentifier	 assert Equal Collation
(	TokenNameLPAREN	
expected	TokenNameIdentifier	 expected
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"foobar"	TokenNameStringLiteral	foobar
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// specify language + country 	TokenNameCOMMENT_LINE	specify language + country 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getCollatorConfig	TokenNameIdentifier	 get Collator Config
(	TokenNameLPAREN	
"en,US"	TokenNameStringLiteral	en,US
,	TokenNameCOMMA	
"impl:jdk"	TokenNameStringLiteral	impl:jdk
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
expected	TokenNameIdentifier	 expected
=	TokenNameEQUAL	
new	TokenNamenew	
CollationKeyAnalyzer	TokenNameIdentifier	 Collation Key Analyzer
(	TokenNameLPAREN	
Collator	TokenNameIdentifier	 Collator
.	TokenNameDOT	
getInstance	TokenNameIdentifier	 get Instance
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
"en"	TokenNameStringLiteral	en
,	TokenNameCOMMA	
"US"	TokenNameStringLiteral	US
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualCollation	TokenNameIdentifier	 assert Equal Collation
(	TokenNameLPAREN	
expected	TokenNameIdentifier	 expected
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"foobar"	TokenNameStringLiteral	foobar
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// specify language + country + variant 	TokenNameCOMMENT_LINE	specify language + country + variant 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getCollatorConfig	TokenNameIdentifier	 get Collator Config
(	TokenNameLPAREN	
"no,NO,NY"	TokenNameStringLiteral	no,NO,NY
,	TokenNameCOMMA	
"impl:jdk"	TokenNameStringLiteral	impl:jdk
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
expected	TokenNameIdentifier	 expected
=	TokenNameEQUAL	
new	TokenNamenew	
CollationKeyAnalyzer	TokenNameIdentifier	 Collation Key Analyzer
(	TokenNameLPAREN	
Collator	TokenNameIdentifier	 Collator
.	TokenNameDOT	
getInstance	TokenNameIdentifier	 get Instance
(	TokenNameLPAREN	
new	TokenNamenew	
Locale	TokenNameIdentifier	 Locale
(	TokenNameLPAREN	
"no"	TokenNameStringLiteral	no
,	TokenNameCOMMA	
"NO"	TokenNameStringLiteral	NO
,	TokenNameCOMMA	
"NY"	TokenNameStringLiteral	NY
)	TokenNameRPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualCollation	TokenNameIdentifier	 assert Equal Collation
(	TokenNameLPAREN	
expected	TokenNameIdentifier	 expected
,	TokenNameCOMMA	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"foobar"	TokenNameStringLiteral	foobar
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
void	TokenNamevoid	
assertEqualCollation	TokenNameIdentifier	 assert Equal Collation
(	TokenNameLPAREN	
Analyzer	TokenNameIdentifier	 Analyzer
a1	TokenNameIdentifier	 a1
,	TokenNameCOMMA	
Analyzer	TokenNameIdentifier	 Analyzer
a2	TokenNameIdentifier	 a2
,	TokenNameCOMMA	
String	TokenNameIdentifier	 String
text	TokenNameIdentifier	 text
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
TokenStream	TokenNameIdentifier	 Token Stream
ts1	TokenNameIdentifier	 ts1
=	TokenNameEQUAL	
a1	TokenNameIdentifier	 a1
.	TokenNameDOT	
tokenStream	TokenNameIdentifier	 token Stream
(	TokenNameLPAREN	
"bogus"	TokenNameStringLiteral	bogus
,	TokenNameCOMMA	
new	TokenNamenew	
StringReader	TokenNameIdentifier	 String Reader
(	TokenNameLPAREN	
text	TokenNameIdentifier	 text
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
TokenStream	TokenNameIdentifier	 Token Stream
ts2	TokenNameIdentifier	 ts2
=	TokenNameEQUAL	
a2	TokenNameIdentifier	 a2
.	TokenNameDOT	
tokenStream	TokenNameIdentifier	 token Stream
(	TokenNameLPAREN	
"bogus"	TokenNameStringLiteral	bogus
,	TokenNameCOMMA	
new	TokenNamenew	
StringReader	TokenNameIdentifier	 String Reader
(	TokenNameLPAREN	
text	TokenNameIdentifier	 text
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ts1	TokenNameIdentifier	 ts1
.	TokenNameDOT	
reset	TokenNameIdentifier	 reset
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ts2	TokenNameIdentifier	 ts2
.	TokenNameDOT	
reset	TokenNameIdentifier	 reset
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
CharTermAttribute	TokenNameIdentifier	 Char Term Attribute
termAtt1	TokenNameIdentifier	 term Att1
=	TokenNameEQUAL	
ts1	TokenNameIdentifier	 ts1
.	TokenNameDOT	
addAttribute	TokenNameIdentifier	 add Attribute
(	TokenNameLPAREN	
CharTermAttribute	TokenNameIdentifier	 Char Term Attribute
.	TokenNameDOT	
class	TokenNameclass	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
CharTermAttribute	TokenNameIdentifier	 Char Term Attribute
termAtt2	TokenNameIdentifier	 term Att2
=	TokenNameEQUAL	
ts2	TokenNameIdentifier	 ts2
.	TokenNameDOT	
addAttribute	TokenNameIdentifier	 add Attribute
(	TokenNameLPAREN	
CharTermAttribute	TokenNameIdentifier	 Char Term Attribute
.	TokenNameDOT	
class	TokenNameclass	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
ts1	TokenNameIdentifier	 ts1
.	TokenNameDOT	
incrementToken	TokenNameIdentifier	 increment Token
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertTrue	TokenNameIdentifier	 assert True
(	TokenNameLPAREN	
ts2	TokenNameIdentifier	 ts2
.	TokenNameDOT	
incrementToken	TokenNameIdentifier	 increment Token
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEquals	TokenNameIdentifier	 assert Equals
(	TokenNameLPAREN	
termAtt1	TokenNameIdentifier	 term Att1
.	TokenNameDOT	
toString	TokenNameIdentifier	 to String
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
termAtt2	TokenNameIdentifier	 term Att2
.	TokenNameDOT	
toString	TokenNameIdentifier	 to String
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertFalse	TokenNameIdentifier	 assert False
(	TokenNameLPAREN	
ts1	TokenNameIdentifier	 ts1
.	TokenNameDOT	
incrementToken	TokenNameIdentifier	 increment Token
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertFalse	TokenNameIdentifier	 assert False
(	TokenNameLPAREN	
ts2	TokenNameIdentifier	 ts2
.	TokenNameDOT	
incrementToken	TokenNameIdentifier	 increment Token
(	TokenNameLPAREN	
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ts1	TokenNameIdentifier	 ts1
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
ts2	TokenNameIdentifier	 ts2
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
getCollatorConfig	TokenNameIdentifier	 get Collator Config
(	TokenNameLPAREN	
String	TokenNameIdentifier	 String
localeParam	TokenNameIdentifier	 locale Param
,	TokenNameCOMMA	
String	TokenNameIdentifier	 String
collationParam	TokenNameIdentifier	 collation Param
)	TokenNameRPAREN	
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"# ----- properties "	TokenNameStringLiteral	# ----- properties 
,	TokenNameCOMMA	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.log.step=3"	TokenNameStringLiteral	content.source.log.step=3
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"# ----- alg "	TokenNameStringLiteral	# ----- alg 
,	TokenNameCOMMA	
"{ "Rounds""	TokenNameStringLiteral	{ "Rounds"
,	TokenNameCOMMA	
" ResetSystemErase"	TokenNameStringLiteral	 ResetSystemErase
,	TokenNameCOMMA	
" NewLocale("	TokenNameStringLiteral	 NewLocale(
+	TokenNamePLUS	
localeParam	TokenNameIdentifier	 locale Param
+	TokenNamePLUS	
")"	TokenNameStringLiteral	)
,	TokenNameCOMMA	
" NewCollationAnalyzer("	TokenNameStringLiteral	 NewCollationAnalyzer(
+	TokenNamePLUS	
collationParam	TokenNameIdentifier	 collation Param
+	TokenNamePLUS	
")"	TokenNameStringLiteral	)
,	TokenNameCOMMA	
" CreateIndex"	TokenNameStringLiteral	 CreateIndex
,	TokenNameCOMMA	
" { "AddDocs" AddDoc > : * "	TokenNameStringLiteral	 { "AddDocs" AddDoc > : * 
,	TokenNameCOMMA	
" NewRound"	TokenNameStringLiteral	 NewRound
,	TokenNameCOMMA	
"} : 1"	TokenNameStringLiteral	} : 1
,	TokenNameCOMMA	
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
return	TokenNamereturn	
algLines	TokenNameIdentifier	 alg Lines
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
/** * Test that we can create ShingleAnalyzerWrappers. */	TokenNameCOMMENT_JAVADOC	 Test that we can create ShingleAnalyzerWrappers. 
public	TokenNamepublic	
void	TokenNamevoid	
testShingleAnalyzer	TokenNameIdentifier	 test Shingle Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
text	TokenNameIdentifier	 text
=	TokenNameEQUAL	
"one,two,three, four five six"	TokenNameStringLiteral	one,two,three, four five six
;	TokenNameSEMICOLON	
// Default analyzer, maxShingleSize, and outputUnigrams 	TokenNameCOMMENT_LINE	Default analyzer, maxShingleSize, and outputUnigrams 
Benchmark	TokenNameIdentifier	 Benchmark
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getShingleConfig	TokenNameIdentifier	 get Shingle Config
(	TokenNameLPAREN	
""	TokenNameStringLiteral	 
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
tokenStream	TokenNameIdentifier	 token Stream
(	TokenNameLPAREN	
"bogus"	TokenNameStringLiteral	bogus
,	TokenNameCOMMA	
new	TokenNamenew	
StringReader	TokenNameIdentifier	 String Reader
(	TokenNameLPAREN	
text	TokenNameIdentifier	 text
)	TokenNameRPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
close	TokenNameIdentifier	 close
(	TokenNameLPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualShingle	TokenNameIdentifier	 assert Equal Shingle
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
text	TokenNameIdentifier	 text
,	TokenNameCOMMA	
new	TokenNamenew	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
{	TokenNameLBRACE	
"one"	TokenNameStringLiteral	one
,	TokenNameCOMMA	
"one two"	TokenNameStringLiteral	one two
,	TokenNameCOMMA	
"two"	TokenNameStringLiteral	two
,	TokenNameCOMMA	
"two three"	TokenNameStringLiteral	two three
,	TokenNameCOMMA	
"three"	TokenNameStringLiteral	three
,	TokenNameCOMMA	
"three four"	TokenNameStringLiteral	three four
,	TokenNameCOMMA	
"four"	TokenNameStringLiteral	four
,	TokenNameCOMMA	
"four five"	TokenNameStringLiteral	four five
,	TokenNameCOMMA	
"five"	TokenNameStringLiteral	five
,	TokenNameCOMMA	
"five six"	TokenNameStringLiteral	five six
,	TokenNameCOMMA	
"six"	TokenNameStringLiteral	six
}	TokenNameRBRACE	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// Default analyzer, maxShingleSize = 3, and outputUnigrams = false 	TokenNameCOMMENT_LINE	Default analyzer, maxShingleSize = 3, and outputUnigrams = false 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getShingleConfig	TokenNameIdentifier	 get Shingle Config
(	TokenNameLPAREN	
"maxShingleSize:3,outputUnigrams:false"	TokenNameStringLiteral	maxShingleSize:3,outputUnigrams:false
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualShingle	TokenNameIdentifier	 assert Equal Shingle
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
text	TokenNameIdentifier	 text
,	TokenNameCOMMA	
new	TokenNamenew	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
{	TokenNameLBRACE	
"one two"	TokenNameStringLiteral	one two
,	TokenNameCOMMA	
"one two three"	TokenNameStringLiteral	one two three
,	TokenNameCOMMA	
"two three"	TokenNameStringLiteral	two three
,	TokenNameCOMMA	
"two three four"	TokenNameStringLiteral	two three four
,	TokenNameCOMMA	
"three four"	TokenNameStringLiteral	three four
,	TokenNameCOMMA	
"three four five"	TokenNameStringLiteral	three four five
,	TokenNameCOMMA	
"four five"	TokenNameStringLiteral	four five
,	TokenNameCOMMA	
"four five six"	TokenNameStringLiteral	four five six
,	TokenNameCOMMA	
"five six"	TokenNameStringLiteral	five six
}	TokenNameRBRACE	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// WhitespaceAnalyzer, default maxShingleSize and outputUnigrams 	TokenNameCOMMENT_LINE	WhitespaceAnalyzer, default maxShingleSize and outputUnigrams 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getShingleConfig	TokenNameIdentifier	 get Shingle Config
(	TokenNameLPAREN	
"analyzer:WhitespaceAnalyzer"	TokenNameStringLiteral	analyzer:WhitespaceAnalyzer
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualShingle	TokenNameIdentifier	 assert Equal Shingle
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
text	TokenNameIdentifier	 text
,	TokenNameCOMMA	
new	TokenNamenew	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
{	TokenNameLBRACE	
"one,two,three,"	TokenNameStringLiteral	one,two,three,
,	TokenNameCOMMA	
"one,two,three, four"	TokenNameStringLiteral	one,two,three, four
,	TokenNameCOMMA	
"four"	TokenNameStringLiteral	four
,	TokenNameCOMMA	
"four five"	TokenNameStringLiteral	four five
,	TokenNameCOMMA	
"five"	TokenNameStringLiteral	five
,	TokenNameCOMMA	
"five six"	TokenNameStringLiteral	five six
,	TokenNameCOMMA	
"six"	TokenNameStringLiteral	six
}	TokenNameRBRACE	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
// WhitespaceAnalyzer, maxShingleSize=3 and outputUnigrams=false 	TokenNameCOMMENT_LINE	WhitespaceAnalyzer, maxShingleSize=3 and outputUnigrams=false 
benchmark	TokenNameIdentifier	 benchmark
=	TokenNameEQUAL	
execBenchmark	TokenNameIdentifier	 exec Benchmark
(	TokenNameLPAREN	
getShingleConfig	TokenNameIdentifier	 get Shingle Config
(	TokenNameLPAREN	
"outputUnigrams:false,maxShingleSize:3,analyzer:WhitespaceAnalyzer"	TokenNameStringLiteral	outputUnigrams:false,maxShingleSize:3,analyzer:WhitespaceAnalyzer
)	TokenNameRPAREN	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
assertEqualShingle	TokenNameIdentifier	 assert Equal Shingle
(	TokenNameLPAREN	
benchmark	TokenNameIdentifier	 benchmark
.	TokenNameDOT	
getRunData	TokenNameIdentifier	 get Run Data
(	TokenNameLPAREN	
)	TokenNameRPAREN	
.	TokenNameDOT	
getAnalyzer	TokenNameIdentifier	 get Analyzer
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
text	TokenNameIdentifier	 text
,	TokenNameCOMMA	
new	TokenNamenew	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
{	TokenNameLBRACE	
"one,two,three, four"	TokenNameStringLiteral	one,two,three, four
,	TokenNameCOMMA	
"one,two,three, four five"	TokenNameStringLiteral	one,two,three, four five
,	TokenNameCOMMA	
"four five"	TokenNameStringLiteral	four five
,	TokenNameCOMMA	
"four five six"	TokenNameStringLiteral	four five six
,	TokenNameCOMMA	
"five six"	TokenNameStringLiteral	five six
}	TokenNameRBRACE	
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
void	TokenNamevoid	
assertEqualShingle	TokenNameIdentifier	 assert Equal Shingle
(	TokenNameLPAREN	
Analyzer	TokenNameIdentifier	 Analyzer
analyzer	TokenNameIdentifier	 analyzer
,	TokenNameCOMMA	
String	TokenNameIdentifier	 String
text	TokenNameIdentifier	 text
,	TokenNameCOMMA	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
expected	TokenNameIdentifier	 expected
)	TokenNameRPAREN	
throws	TokenNamethrows	
Exception	TokenNameIdentifier	 Exception
{	TokenNameLBRACE	
BaseTokenStreamTestCase	TokenNameIdentifier	 Base Token Stream Test Case
.	TokenNameDOT	
assertAnalyzesTo	TokenNameIdentifier	 assert Analyzes To
(	TokenNameLPAREN	
analyzer	TokenNameIdentifier	 analyzer
,	TokenNameCOMMA	
text	TokenNameIdentifier	 text
,	TokenNameCOMMA	
expected	TokenNameIdentifier	 expected
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
String	TokenNameIdentifier	 String
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
getShingleConfig	TokenNameIdentifier	 get Shingle Config
(	TokenNameLPAREN	
String	TokenNameIdentifier	 String
params	TokenNameIdentifier	 params
)	TokenNameRPAREN	
{	TokenNameLBRACE	
String	TokenNameIdentifier	 String
algLines	TokenNameIdentifier	 alg Lines
[	TokenNameLBRACKET	
]	TokenNameRBRACKET	
=	TokenNameEQUAL	
{	TokenNameLBRACE	
"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource"	TokenNameStringLiteral	content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
,	TokenNameCOMMA	
"docs.file="	TokenNameStringLiteral	docs.file=
+	TokenNamePLUS	
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
,	TokenNameCOMMA	
"content.source.forever=false"	TokenNameStringLiteral	content.source.forever=false
,	TokenNameCOMMA	
"directory=RAMDirectory"	TokenNameStringLiteral	directory=RAMDirectory
,	TokenNameCOMMA	
"NewShingleAnalyzer("	TokenNameStringLiteral	NewShingleAnalyzer(
+	TokenNamePLUS	
params	TokenNameIdentifier	 params
+	TokenNamePLUS	
")"	TokenNameStringLiteral	)
,	TokenNameCOMMA	
"CreateIndex"	TokenNameStringLiteral	CreateIndex
,	TokenNameCOMMA	
"{ "AddDocs" AddDoc > : * "	TokenNameStringLiteral	{ "AddDocs" AddDoc > : * 
}	TokenNameRBRACE	
;	TokenNameSEMICOLON	
return	TokenNamereturn	
algLines	TokenNameIdentifier	 alg Lines
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
private	TokenNameprivate	
String	TokenNameIdentifier	 String
getReuters20LinesFile	TokenNameIdentifier	 get Reuters20 Lines File
(	TokenNameLPAREN	
)	TokenNameRPAREN	
{	TokenNameLBRACE	
return	TokenNamereturn	
getWorkDirResourcePath	TokenNameIdentifier	 get Work Dir Resource Path
(	TokenNameLPAREN	
"reuters.first20.lines.txt"	TokenNameStringLiteral	reuters.first20.lines.txt
)	TokenNameRPAREN	
;	TokenNameSEMICOLON	
}	TokenNameRBRACE	
}	TokenNameRBRACE	
